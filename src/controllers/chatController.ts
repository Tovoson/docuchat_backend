import { type Request, type Response } from "express";
import { GoogleGenerativeAIEmbeddings } from "@langchain/google-genai";
import { GoogleGenerativeAI } from "@google/generative-ai";
import { TaskType } from "@google/generative-ai";
import sql from "../config/db.js"; // Import de la connexion Supabase

// Initialisation lazy ou globale s√©curis√©e du mod√®le Gemini
const getGenerativeModel = () => {
  const apiKey = process.env.GOOGLE_API_KEY;
  if (!apiKey) {
    throw new Error("GOOGLE_API_KEY is missing in environment variables");
  }
  const genAI = new GoogleGenerativeAI(apiKey);
  // Utilisation de 'gemini-flash-latest' pour √©viter les erreurs de quota/version sur les mod√®les exp√©rimentaux
  return genAI.getGenerativeModel({ model: "gemini-flash-latest" });
};

// Fonction pour r√©pondre aux questions en utilisant RAG (Retrieval-Augmented Generation)
export const askQuestion = async (req: Request, res: Response) => {
  try {
    const { question } = req.body;
    const model = getGenerativeModel();

    if (!question) {
      return res.status(400).json({ error: "La question est requise" });
    }

    console.log(`‚ùì [Chat] Question re√ßue : "${question}"`);

    // 1. G√©n√©ration de l'embedding de la question avec Gemini
    console.log("üîÑ G√©n√©ration de l'embedding...");
    const embeddings = new GoogleGenerativeAIEmbeddings({
      apiKey: process.env.GOOGLE_API_KEY || "",
      modelName: "gemini-embedding-001",
      taskType: TaskType.RETRIEVAL_QUERY,
    });

    const questionVector = await embeddings.embedQuery(question);
    console.log(`üìè Vecteur question g√©n√©r√© (${questionVector.length} dims)`);

    // 2. Recherche vectorielle dans Supabase (Cosine Similarity)
    console.log("üîç Recherche dans la base de donn√©es...");
    // On r√©cup√®re les 3 morceaux les plus proches en utilisant l'op√©rateur de distance cosinus
    const searchResult = await sql`
      SELECT content, 1 - (embedding <=> ${JSON.stringify(questionVector)}) as similarity
      FROM documents
      ORDER BY embedding <=> ${JSON.stringify(questionVector)}
      LIMIT 3
    `;

    // Extraction du contexte √† partir des r√©sultats de la recherche
    const context = searchResult.map((row: any) => row.content).join("\n\n");
    console.log(`üìö Contexte trouv√© (${searchResult.length} morceaux)`);

    // 3. Construire le prompt pour Gemini avec le contexte r√©cup√©r√©
    console.log("ü§ñ G√©n√©ration de la r√©ponse avec Gemini...");
    const prompt = `Voici des informations contextuelles extraites d'un document PDF :
    
    ${context}
    
    En utilisant UNIQUEMENT ce contexte, r√©ponds √† la question suivante :
    Question : ${question}
    
    Si la r√©ponse n'est pas dans le contexte, dis simplement que tu ne sais pas.`;

    // 4. G√©n√©rer la r√©ponse avec Gemini
    const result = await model.generateContent(prompt);
    const response = result.response;
    const answer = response.text();

    console.log("‚úÖ R√©ponse g√©n√©r√©e avec succ√®s");

    // Retourner la r√©ponse avec les sources utilis√©es
    res.json({
      answer,
      sources: searchResult.map((row: any) => ({
        content: row.content.substring(0, 100) + "...",
        similarity: row.similarity,
      })),
    });
  } catch (error: any) {
    console.error("‚ùå [Chat] Erreur d√©taill√©e :", error);
    if (error.response) {
      console.error("   D√©tails API :", error.response.data);
    }
    res.status(500).json({
      error: error.message || "Erreur lors de la g√©n√©ration de la r√©ponse",
    });
  }
};
